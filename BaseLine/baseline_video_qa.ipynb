{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSIib9XGY6Fs"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yHciHyC2bDYS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mlh07tT9fuc6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8m4D94FWcTq6"
   },
   "outputs": [],
   "source": [
    "!unzip aa.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0HEiMnKroae"
   },
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jAGDCFZ6aOCA"
   },
   "outputs": [],
   "source": [
    "import clip\n",
    "import os\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as nnf\n",
    "import sys\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from tqdm import tqdm, trange\n",
    "import PIL.Image\n",
    "from IPython.display import Image \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "85FkKUgCaQLz"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, sizes, bias=True, act=nn.Tanh):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(sizes) -1):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
    "            if i < len(sizes) - 2:\n",
    "                layers.append(act())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "class ClipCaptionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, prefix_length, prefix_size: int = 768):\n",
    "        super(ClipCaptionModel, self).__init__()\n",
    "        self.prefix_length = prefix_length\n",
    "\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained('sberbank-ai/rugpt3large_based_on_gpt2')\n",
    "        \n",
    "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
    "        \n",
    "        if prefix_length > 10:  # not enough memory\n",
    "            self.clip_project = nn.Linear(10,47)#prefix_size, self.gpt_embedding_size * prefix_length)\n",
    "        else:\n",
    "            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2, self.gpt_embedding_size * prefix_length))\n",
    "\n",
    "    #@functools.lru_cache #FIXME\n",
    "    def get_dummy_token(self, batch_size, device):\n",
    "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
    "\n",
    "    def forward(self, tokens, prefix, mask, labels):\n",
    "        embedding_text = self.gpt.transformer.wte(tokens)\n",
    "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
    "        #print(embedding_text.size()) #torch.Size([5, 67, 768])\n",
    "        #print(prefix_projections.size()) #torch.Size([5, 1, 768])\n",
    "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
    "        if labels is not None:\n",
    "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
    "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
    "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
    "        return out\n",
    "\n",
    "    \n",
    "\n",
    "class ClipCaptionPrefix(ClipCaptionModel):\n",
    "\n",
    "    def parameters(self, recurse = True):\n",
    "        return self.clip_project.parameters()\n",
    "\n",
    "    def train(self, mode = True):\n",
    "        super(ClipCaptionPrefix, self).train(mode)\n",
    "        self.gpt.eval()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BGtgf3oaSRG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_Z2-Uaiampr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "S21C4pkyarFN"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('ru_train_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nRjeRPvia6uv"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "data = []\n",
    "for video_name, question, answer in zip(df_train.video_name, df_train.question, df_train.answer):\n",
    "    name = f'videos/{video_name}.mp4'\n",
    "    if os.path.exists(name):  \n",
    "        data += [(name,f'Q: {question} A: {answer}')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Bjj0uB4gccCS"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import PIL\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import transformers\n",
    "import more_itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass, field\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import cv2\n",
    "import clip\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from tqdm.contrib import tzip\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    pils = imgs\n",
    "    \n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "def read_video(path, transform=None, frames_num=16, window=30):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    \n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    N = length//(frames_num)\n",
    "    #print(length)\n",
    "    #counter = \n",
    "    \n",
    "    current_frame = 1\n",
    "    for i in range(length):\n",
    "    \n",
    "        #frameId = int(round(cap.get(current_frame))) \n",
    "        #print(current_frame)\n",
    "        ret, frame = cap.read(current_frame)\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if ret and i==current_frame and len(frames)<frames_num:\n",
    "            size = 64, 64\n",
    "            frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            frame.thumbnail(size, Image.ANTIALIAS)\n",
    "            \n",
    "            frames.append(frame)\n",
    "            current_frame += N\n",
    "        \n",
    "       \n",
    "        #print(current_frame)\n",
    "        #cap.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
    "        \n",
    "        \n",
    "    cap.release()\n",
    "    #print(frames)\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0uR5W2TTccdv"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "clip_model_type = \"ViT-L/14@336px\"\n",
    "\n",
    "out_path = f\"Features_train_full_ru.pkl\"\n",
    "video_path =  'videos'\n",
    "\n",
    "\n",
    "clip_model, preprocess = clip.load(clip_model_type, device=device, jit=False)\n",
    "\n",
    "# path_a = 'activitynet-qa/dataset/train_a.json'\n",
    "# path_q = 'activitynet-qa/dataset/train_q.json'\n",
    "# df_a = pd.read_json(path_a)\n",
    "# df_q = pd.read_json(path_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0fBnjfERcjlp"
   },
   "outputs": [],
   "source": [
    "clip_model.to(device)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71,
     "referenced_widgets": [
      "f84af7dfce4a4153a907e52b1a0ff3fe"
     ]
    },
    "id": "kwOK5wascju4",
    "outputId": "809def4a-2b3c-45ed-b429-cb242219bc5d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7509c0709a4834a6e8fc708a18be8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_embeddings = []\n",
    "all_captions = []\n",
    "i = 0\n",
    "\n",
    "for video_name, question, answer in tzip(df_train.video_name, df_train.question, df_train.answer):\n",
    "    \n",
    "    \n",
    "    name = f'{video_path}/{video_name}.mp4'\n",
    "    \n",
    "    text = f'Q: {question} A: {answer}'\n",
    "    #print(name)\n",
    "    if os.path.exists(name):\n",
    "        \n",
    "        video = read_video(path = name, frames_num=9)\n",
    "        if len(video)>1:\n",
    "            #print(len(video))\n",
    "            image = image_grid(video,3,3)\n",
    "\n",
    "            image = preprocess(image).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                prefix = clip_model.encode_image(image).cpu()\n",
    "            #d[\"clip_embedding\"] = i\n",
    "            all_embeddings.append(prefix)\n",
    "            all_captions.append(text)\n",
    "    \n",
    "with open(out_path, 'wb') as f:\n",
    "    pickle.dump({\"clip_embedding\": torch.cat(all_embeddings, dim=0), \"captions\": all_captions}, f)\n",
    "\n",
    "print('Done')\n",
    "print(\"%0d embeddings saved \" % len(all_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5uF07hEqcltu"
   },
   "outputs": [],
   "source": [
    "\n",
    "import gc\n",
    "import io\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torchvision\n",
    "import transformers\n",
    "import more_itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass, field\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import sys\n",
    "from tqdm.contrib import tzip\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as nnf\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import argparse\n",
    "import json\n",
    "from typing import Tuple, Optional, Union\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "import wandb\n",
    "import torch\n",
    "\n",
    "from torch.utils.checkpoint import checkpoint_sequential\n",
    "\n",
    "class ClipCocoDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_path: str,  prefix_length= 50, gpt2_type = \"sberbank-ai/rugpt3large_based_on_gpt2\",\n",
    "                 normalize_prefix=False):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "        self.prefix_length = prefix_length\n",
    "        self.normalize_prefix = normalize_prefix\n",
    "        with open(data_path, 'rb') as f:\n",
    "            all_data = pickle.load(f)\n",
    "        print(\"Data size is %0d\" % len(all_data[\"clip_embedding\"]))\n",
    "        sys.stdout.flush()\n",
    "        self.prefixes = all_data[\"clip_embedding\"]\n",
    "        captions_raw = all_data[\"captions\"]\n",
    "        \n",
    "        #self.image_ids = [caption[\"image_id\"] for caption in captions_raw]\n",
    "        \n",
    "        self.captions = captions_raw\n",
    "        \n",
    "        \n",
    "        self.captions_tokens = []\n",
    "        self.caption2embedding = []\n",
    "        max_seq_len = 0\n",
    "        i=0\n",
    "        for caption in tqdm(captions_raw):\n",
    "                self.captions_tokens.append(torch.tensor(self.tokenizer.encode(caption), dtype=torch.int64))\n",
    "                self.caption2embedding.append(self.prefixes[i])\n",
    "                i+=1\n",
    "                max_seq_len = max(max_seq_len, self.captions_tokens[-1].shape[0])\n",
    "            # self.max_seq_len = max_seq_len\n",
    "        #del self.captions_tokens\n",
    "        #del self.caption2embedding\n",
    "        #gc.collect()\n",
    "        #with open(f\"{data_path[:-4]}_tokens.pkl\", 'wb') as f:\n",
    "        #        pickle.dump([self.captions_tokens, self.caption2embedding, max_seq_len], f)\n",
    "       \n",
    "    \n",
    "    \n",
    "        all_len = torch.tensor([len(self.captions_tokens[i]) for i in range(len(self))]).float()\n",
    "        self.max_seq_len = min(int(all_len.mean() + all_len.std() * 10), int(all_len.max()))\n",
    "\n",
    "    def pad_tokens(self, item: int):\n",
    "        tokens = self.captions_tokens[item]\n",
    "        padding = self.max_seq_len - tokens.shape[0]\n",
    "        if padding > 0:\n",
    "            tokens = torch.cat((tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n",
    "            self.captions_tokens[item] = tokens\n",
    "        elif padding < 0:\n",
    "            tokens = tokens[:self.max_seq_len]\n",
    "            self.captions_tokens[item] = tokens\n",
    "        mask = tokens.ge(0)  # mask is zero where we out of sequence\n",
    "        tokens[~mask] = 0\n",
    "        mask = mask.float()\n",
    "        mask = torch.cat((torch.ones(self.prefix_length), mask), dim=0)  # adding prefix mask\n",
    "        return tokens, mask\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.captions_tokens)\n",
    "\n",
    "   \n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        tokens, mask = self.pad_tokens(item)\n",
    "        prefix = self.prefixes[item]\n",
    "        if self.normalize_prefix:\n",
    "            prefix = prefix.float()\n",
    "            prefix = prefix / prefix.norm(2, -1)\n",
    "        return tokens, mask, prefix\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "H6d-G11Bc-Lb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size is 27570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27570/27570 [00:03<00:00, 7264.12it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = ClipCocoDataset('Features_train_full_ru.pkl', prefix_length=50, normalize_prefix=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "51d4XF65f7Eh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "! e461a6a3bca9f7cec3390a40dc10cdf576ce3252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160
    },
    "id": "K3N3MQ7afj8m",
    "outputId": "986bd87f-084a-470f-e25f-953e760b389d"
   },
   "outputs": [],
   "source": [
    "\n",
    "wandb.init(project=\"clip_caption_video\")\n",
    "\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 1):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
    "            if i < len(sizes) - 2:\n",
    "                layers.append(act())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    #@autocast()  \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    \n",
    "def freeze(\n",
    "    model,\n",
    "    freeze_emb=False,\n",
    "    freeze_ln=False,\n",
    "    freeze_attn=True,\n",
    "    freeze_ff=True,\n",
    "    freeze_other=True,\n",
    "):\n",
    "    \n",
    "    for name, p in model.named_parameters():\n",
    "    # freeze all parameters except the layernorm and positional embeddings\n",
    "       \n",
    "       \n",
    "        \n",
    "        name = name.lower()\n",
    "        if 'ln' in name or 'norm' in name:\n",
    "            p.requires_grad = not freeze_ln\n",
    "        elif 'embeddings' in name:\n",
    "            p.requires_grad = not freeze_emb\n",
    "        elif 'mlp' in name:\n",
    "            p.requires_grad = not freeze_ff\n",
    "        elif 'attn' in name:\n",
    "            p.requires_grad = not freeze_attn\n",
    "        else:\n",
    "            p.requires_grad = not freeze_other\n",
    "           \n",
    "    return model\n",
    "\n",
    "class ClipCaptionModel(nn.Module):\n",
    "    def __init__(self, backbone, prefix_length: int, prefix_size: int = 768):\n",
    "          super(ClipCaptionModel, self).__init__()\n",
    "          self.prefix_length = prefix_length\n",
    "          \"\"\"\n",
    "          ru gpts shit\n",
    "          \n",
    "          \"\"\"\n",
    "          self.gpt = GPT2LMHeadModel.from_pretrained(backbone)\n",
    "          #self.gpt = freeze(self.gpt)\n",
    "          self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
    "          self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2,\n",
    "                                  self.gpt_embedding_size * prefix_length))\n",
    "\n",
    "    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n",
    "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
    "    \n",
    "    # @autocast() \n",
    "    def forward(self, tokens: torch.Tensor, prefix: torch.Tensor, mask: Optional[torch.Tensor] = None,\n",
    "                labels: Optional[torch.Tensor] = None):\n",
    "\n",
    "        embedding_text = self.gpt.transformer.wte(tokens)\n",
    "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
    "\n",
    "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
    "        if labels is not None:\n",
    "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
    "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
    "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
    "        return out\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "class ClipCaptionPrefix(ClipCaptionModel):\n",
    "\n",
    "    def parameters(self, recurse: bool = True):\n",
    "        return self.clip_project.parameters()\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        super(ClipCaptionPrefix, self).train(mode)\n",
    "        self.gpt.eval()\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(dataset, model: ClipCaptionModel, args,\n",
    "          warmup_steps: int = 5000, output_dir: str = \".\", output_prefix: str = \"\"):\n",
    "\n",
    "    device = torch.device('cuda')# xm.xla_device()\n",
    "    #\n",
    "    batch_size = args.bs\n",
    "    epochs = args.epochs\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    model = freeze(model)\n",
    "    model.train()\n",
    "    optimizer = AdamW(model.parameters(), lr=args.lr,betas=(0.9, 0.995))\n",
    "    #optimizer = bnb.optim.Adam8bit(model.parameters(), lr=0.001, betas=(0.9, 0.995))\n",
    "    #optimizer = SM3(model.parameters(),lr=args.lr)\n",
    "    #Adafactor(model.parameters(),scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n",
    "\n",
    "    train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=epochs * len(train_dataloader)\n",
    "    )\n",
    "    #AdafactorSchedule(optimizer)#num_training_steps=epochs * len(train_dataloader\n",
    "    #save_config(args)\n",
    "    #print\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\">>> Training epoch {epoch}\")\n",
    "        sys.stdout.flush()\n",
    "        progress = tqdm(total=len(train_dataloader), desc=output_prefix)\n",
    "        step=0\n",
    "        for idx, (tokens, mask, prefix) in enumerate(train_dataloader):\n",
    "            model.zero_grad()\n",
    "            step+=1\n",
    "            tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.float32)\n",
    "            \n",
    "            outputs = model(tokens, prefix, mask)\n",
    "            logits = outputs.logits[:, dataset.prefix_length - 1: -1]\n",
    "\n",
    "            loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokens.flatten(), ignore_index=0)\n",
    "\n",
    "            segments = 2\n",
    "\n",
    "           \n",
    "            #out = checkpoint_sequential(modules, segments, input_var)\n",
    "\n",
    "            # backpropagate\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            #optimizer.zero_grad()\n",
    "            progress.set_postfix({\"loss\": loss.item()})\n",
    "            \n",
    "            clipping_value = 0.5 # arbitrary value of your choosing\n",
    "            #torch.nn.utils.clip_grad_norm(model.parameters(), clipping_value)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            wandb.log({\"loss\":  loss.item()})\n",
    "            \n",
    "            progress.update()\n",
    "            \n",
    "\n",
    "            del tokens\n",
    "            del mask\n",
    "            del prefix\n",
    "            torch.clear_autocast_cache()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            if (idx + 1) % 7000 == 0:\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    \n",
    "                    os.path.join(output_dir, f\"{output_prefix}_latest.pt\"),\n",
    "                )\n",
    "        progress.close()\n",
    "        if epoch % args.save_every ==0:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(output_dir, f\"{output_prefix}-{epoch:03d}.pt\"),\n",
    "            )\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.backbone = 'sberbank-ai/rugpt3small_based_on_gpt2'\n",
    "        self.data = 'Features_train_full_ru.pkl'\n",
    "        self.out_dir = 'checkpoints_larger'\n",
    "        self.prefix = 'prefix_1'\n",
    "        self.epochs = 10\n",
    "        self.save_every = 1\n",
    "        self.prefix_length = 50\n",
    "        self.bs = 20\n",
    "        self.only_prefix = False\n",
    "        self.lr = 5e-5\n",
    "        \n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    \n",
    "    args = Args()\n",
    "    wandb.config = {\n",
    "      \"learning_rate\": args.lr,\n",
    "      \"epochs\": args.epochs,\n",
    "      \"batch_size\": args.bs\n",
    "    }\n",
    "\n",
    "    prefix_length = args.prefix_length\n",
    "\n",
    "    dataset = ClipCocoDataset(args.data, prefix_length)\n",
    "    \n",
    "   \n",
    "    #model_path = 'prefix_1-003.pt'\n",
    "    model = ClipCaptionModel(backbone = 'sberbank-ai/rugpt3large_based_on_gpt2', prefix_length = 50)\n",
    "    # model.load_state_dict(torch.load(model_path, map_location='cpu')) \n",
    "    print(\"Train both prefix and GPT\")\n",
    "    sys.stdout.flush()\n",
    "    train(dataset, model, args, output_dir=args.out_dir, output_prefix=args.prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TqnMOi63Hkbs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R9cI3gxHhDY6"
   },
   "outputs": [],
   "source": [
    "!rm -r checkpoints*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qT6UdHwPhJGa",
    "outputId": "da69f7af-2734-47a8-9c97-67e69e6b2fd9"
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyO_SyvJAepw"
   },
   "outputs": [],
   "source": [
    "!cp /content/checkpoints/prefix_1-007.pt /content/drive/MyDrive/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_hej1zYCopu"
   },
   "outputs": [],
   "source": [
    "!cp /content/drive/MyDrive/4/prefix_1-007.pt ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "doKt6VnHAMc5"
   },
   "outputs": [],
   "source": [
    "!pip install tg-logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FxwaqhsMHmM5"
   },
   "outputs": [],
   "source": [
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sqNobiKVGTVi"
   },
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "clip_model, preprocess = clip.load(\"ViT-L/14@336px\", device=device, jit=False)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('sberbank-ai/rugpt3large_based_on_gpt2')\n",
    "prefix_length= 50\n",
    "model_path = 'prefix_1-007.pt'\n",
    "model = ClipCaptionModel(backbone = 'gpt2', prefix_length = 50)\n",
    "model.load_state_dict(torch.load(model_path, map_location='cpu')) \n",
    "model.to(device)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VnGs2uMtH3IW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cl1Kh03qhq3-"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import PIL\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import transformers\n",
    "import more_itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "#from tqdm import tqdm\n",
    "from dataclasses import dataclass, field\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import cv2\n",
    "from PIL import Image\n",
    "def image_grid(imgs, rows, cols):\n",
    "    pils = imgs\n",
    "    \n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "def read_video(path, transform=None, frames_num=9, window=30):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    \n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    N = length//(frames_num)\n",
    "    #print(length)\n",
    "    #counter = \n",
    "    \n",
    "    current_frame = 1\n",
    "    for i in range(length):\n",
    "    \n",
    "        #frameId = int(round(cap.get(current_frame))) \n",
    "        #print(current_frame)\n",
    "        ret, frame = cap.read(current_frame)\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if ret and i==current_frame and len(frames)<frames_num:\n",
    "            size = 193, 193\n",
    "            frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            frame.thumbnail(size, Image.ANTIALIAS)\n",
    "            \n",
    "            frames.append(frame)\n",
    "            current_frame += N\n",
    "        \n",
    "       \n",
    "        #print(current_frame)\n",
    "        #cap.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
    "        \n",
    "        \n",
    "    cap.release()\n",
    "    #print(frames)\n",
    "    return frames\n",
    "\n",
    "\n",
    "\n",
    "def filter_ngrams(output_text):\n",
    "    a_pos = output_text.find(' A:')\n",
    "    sec_a_pos = output_text.find(' A:', a_pos + 1)\n",
    "    \n",
    "    return output_text[:sec_a_pos]\n",
    "\n",
    "def generate2(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        tokens=None,\n",
    "        prompt='',\n",
    "        embed=None,\n",
    "        entry_count=1,\n",
    "        entry_length=67,  # maximum number of words\n",
    "        top_p=0.98,\n",
    "        temperature=1.,\n",
    "        stop_token = '.',\n",
    "):\n",
    "    model.eval()\n",
    "    generated_num = 0\n",
    "    generated_list = []\n",
    "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
    "    filter_value = -float(\"Inf\")\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for entry_idx in range(entry_count):\n",
    "            if not tokens:\n",
    "                    tokens = torch.tensor(tokenizer.encode(prompt))\n",
    "                    #print('tokens',tokens)\n",
    "                    tokens = tokens.unsqueeze(0).to(device)\n",
    "                    \n",
    "            emb_tokens = model.gpt.transformer.wte(tokens)\n",
    "            \n",
    "            if embed is not None:\n",
    "                generated = torch.cat((embed, emb_tokens), dim=1)\n",
    "            else:\n",
    "                generated = emb_tokens\n",
    "\n",
    "            for i in range(entry_length):\n",
    "\n",
    "                outputs = model.gpt(inputs_embeds=generated)\n",
    "                logits = outputs.logits\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(nnf.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                                                    ..., :-1\n",
    "                                                    ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "                #\n",
    "                top_k = 2000 \n",
    "                top_p = 0.98\n",
    "                #print(logits)\n",
    "                #next_token = transformers.top_k_top_p_filtering(logits.to(torch.int64).unsqueeze(0), top_k=top_k, top_p=top_p)\n",
    "                next_token = torch.argmax(logits, -1).unsqueeze(0)\n",
    "                next_token_embed = model.gpt.transformer.wte(next_token)\n",
    "\n",
    "                if tokens is None:\n",
    "                    tokens = next_token\n",
    "                else:\n",
    "                    tokens = torch.cat((tokens, next_token), dim=1)\n",
    "                generated = torch.cat((generated, next_token_embed), dim=1)\n",
    "               \n",
    "                if stop_token_index == next_token.item():\n",
    "                    break\n",
    "\n",
    "            output_list = list(tokens.squeeze().cpu().numpy())\n",
    "            \n",
    "            output_text = tokenizer.decode(output_list)\n",
    "            output_text = filter_ngrams(output_text)\n",
    "            generated_list.append(output_text)\n",
    "\n",
    "    return generated_list[0]\n",
    "#from tqdm import tqdm, trange\n",
    "\n",
    "\n",
    "def _to_caption(pil_image,prompt=''):\n",
    "    device = 'cuda:0'\n",
    "    image = preprocess(pil_image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)\n",
    "        prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\n",
    "        if prompt:\n",
    "            generated_text_prefix = generate2(model, tokenizer, prompt=prompt, embed=prefix_embed)\n",
    "        else:\n",
    "            generated_text_prefix = generate2(model, tokenizer, embed=prefix_embed)\n",
    "    return generated_text_prefix.replace('\\n',' ').replace('\\xa0','')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qlhoXxtbALdK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekZVTiP0IC9r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wV7qGc8ODxEF"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import datetime\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import json\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "\n",
    "import pytz\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "from telebot import types\n",
    "import tg_logger\n",
    "import logging\n",
    "import telebot \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def unique_list(l):\n",
    "    ulist = []\n",
    "    [ulist.append(x) for x in l if x not in ulist]\n",
    "    return ulist\n",
    "\n",
    "\n",
    "boot_time = time.time()\n",
    "boot_date = datetime.datetime.now(tz=pytz.timezone(\"Europe/Moscow\"))\n",
    "\n",
    "# ------------- flask config -------------\n",
    "\n",
    "\n",
    "# ------------- bot config -------------\n",
    "WEBHOOK_TOKEN = 'aa'\n",
    "BOT_TOKEN = '5676745030:AAEcXUG-wF-IMBTbDFl11ZhXwlkVKqOvaMM'\n",
    "bot = telebot.TeleBot(BOT_TOKEN)\n",
    "\n",
    "# ------------- log ---------------\n",
    "users = ['241154130']\n",
    "\n",
    "alpha_logger = logging.getLogger()\n",
    "alpha_logger.setLevel(logging.INFO)\n",
    "tg_logger.setup(alpha_logger, token=\"1227347441:AAEnih283opCWcQLFcbghBXc_t1tIp64QXA\", users=users)\n",
    "\n",
    "logger = logging.getLogger(\"tg-bot-tti\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@bot.message_handler(commands=['help', 'start'])\n",
    "def say_welcome(message):\n",
    "    '''Displaying the bot's start interface'''\n",
    "\n",
    "    logger.info(f'</code>@{message.from_user.username}<code> ({message.chat.id}) used /start or /help')\n",
    "    bot.send_message(message.chat.id,\n",
    "                     \"\"\" Text2Image  generate faces here \"\"\",\n",
    "                     parse_mode='html')\n",
    "\n",
    "#from PIL import Image\n",
    "#@bot.message_handler(content_types=['video'])\n",
    "#def get_file(message):\n",
    "#    file_name = message.json['video']['file_name']\n",
    "#    file_info = bot.get_file(message.video.file_id)\n",
    "#    with open(file_name, \"wb\") as f:\n",
    "#        file_content = bot.download_file(file_info.file_path)\n",
    "#        f.write(file_content)\n",
    "#   bot.reply_to(message, f\"OK. Сохранил {file_name}\")\n",
    "\n",
    "\n",
    "@bot.message_handler(content_types=['video'])\n",
    "def photo(message):\n",
    "    q = message.caption\n",
    "    \n",
    "    logger.info(f'{message.from_user.username} {q}')\n",
    "    file_name = message.json['video']['file_name']\n",
    "    file_info = bot.get_file(message.video.file_id)\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        file_content = bot.download_file(file_info.file_path)\n",
    "\n",
    "        f.write(file_content)\n",
    "    try:\n",
    "        path = file_name\n",
    "\n",
    "\n",
    "        video = read_video(path = path, transform = None,frames_num=4)\n",
    "        i = image_grid (video,2,2)\n",
    "        ans = _to_caption(i, prompt=f'Q:{q} A:')\n",
    "\n",
    "        #image = PIL.Image.open(\"image.jpg\")\n",
    "        #ans = _to_caption(image).replace('<|endoftext|>','')\n",
    "        #ans = ' '.join(unique_list(ans.split()))\n",
    "        #print(f'{message.from_user.username} {ans}')\n",
    "        logger.info(f'{message.from_user.username} {ans}')\n",
    "        #bot.send_message(message.chat.id,ans)\n",
    "        #class_ = clf(image)[0]\n",
    "        bot.send_message(message.chat.id, f'{ans.split(\"A:\")[1]}' )\n",
    "    except Exception as e:\n",
    "         bot.send_message(message.chat.id, e )\n",
    "\n",
    "        \n",
    "        \n",
    "      \n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "  \n",
    "  bot.polling(none_stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijbsQSPJFINb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
